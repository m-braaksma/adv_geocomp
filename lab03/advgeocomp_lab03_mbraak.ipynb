{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computation Lab\n",
    "\n",
    "(40 points)\n",
    "\n",
    "This lab will expand upon the in-class activity in the Parallel Computation lab. Process is a method that executes a targeted function such as greeting with a set of arguments. While a developer has full control over the parallel execution of their Process (or Processes), they have to individually manage each one using start() and join() procedures. That can get tedious quickly.\n",
    "\n",
    "So we learned about Map (and MapReduce in lecture), which allows a function to be applied a list of elements in parallel. The benefit is that we don't have to manage the parallel execution, we only have to setup a function to be mapped to a list of things. The downside is that we lose the customization that we could have gotten in Process. So there is a tradeoff. However, Map has a lot of uses in GIS when applied to a list of raster cells or vector features or files (depending on the level of parallelism, which is also called the granularity of parallelism).\n",
    "\n",
    "In this lab, we will experiment with incorporating parallelism at different levels of a geospatial problem using map and reduce tasks. You will explore the performance benefits or detriments of adding or subtracting parallelism using a generic case study. These codes can then be used for a variety of GIS problems later in your career if a need for parallelism ever comes up.\n",
    "\n",
    "Using the template Jupyter Notebook examine the performance trade-offs of parallelism. I would encourage everyone to try running these experiments on notebooks.msi.umn.edu. However, you are free to run them on your own computer/laptop/environment. Just please document where you were running it, because it could influence the interpretation of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Matt Braaksma\n",
    "### Email: braak014@umn.edu\n",
    "\n",
    "### Where are you running this notebook?\n",
    "My personal computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores on this machine 12\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(\"Number of cores on this machine\", cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Information:\n",
      "User time: 130044.66\n",
      "System time: 66989.96\n",
      "Idle time: 1802627.64\n",
      "CPU Usage: 23.7%\n",
      "\n",
      "Memory Information:\n",
      "Total memory: 16.00 GB\n",
      "Available memory: 3.47 GB\n",
      "Memory usage: 78.3%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get CPU information\n",
    "cpu_info = psutil.cpu_times()\n",
    "cpu_percent = psutil.cpu_percent()\n",
    "\n",
    "print(\"CPU Information:\")\n",
    "print(f\"User time: {cpu_info.user}\")\n",
    "print(f\"System time: {cpu_info.system}\")\n",
    "print(f\"Idle time: {cpu_info.idle}\")\n",
    "print(f\"CPU Usage: {cpu_percent}%\")\n",
    "\n",
    "# Get memory information\n",
    "memory_info = psutil.virtual_memory()\n",
    "\n",
    "print(\"\\nMemory Information:\")\n",
    "print(f\"Total memory: {memory_info.total / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Available memory: {memory_info.available / (1024 ** 3):.2f} GB\")\n",
    "print(f\"Memory usage: {memory_info.percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Code\n",
    "Please execute the following code cell to create 25 rasters that are 5,000 x 5,000 cells each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we had a bunch of raster tiles?\n",
    "# Let's make some!!\n",
    "nfiles = 8\n",
    "nrows = 2000 \n",
    "ncols = 2000\n",
    "\n",
    "def maketiles(nfiles,nrows,ncols):\n",
    "   for f_i in range(nfiles):\n",
    "        f = open(\"tmp_raster\"+str(f_i)+\".asc\",\"w\")\n",
    "        f.write(\"ncols \"+str(ncols)+\"\\n\")\n",
    "        f.write(\"nrows \"+str(nrows)+\"\\n\")\n",
    "        f.write(\"xllcorner 0.0\\n\")\n",
    "        f.write(\"yllcorner 0.0\\n\")\n",
    "        f.write(\"cellsize 1.0\\n\")\n",
    "        f.write(\"NODATA_value -999\\n\")\n",
    "        \n",
    "        for i in range(nrows):\n",
    "            for j in range(ncols):\n",
    "                f.write(str(i+j+f_i)+\" \")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.close()\n",
    "\n",
    "maketiles(nfiles,nrows,ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Code\n",
    "The code cell below is the template that we will be using. This is the 'serial version' of the code that we will be manipulating to add parallelism. Start your exploration by timing the execution of this cell when using serial processing. This becomes 'the time to beat' when adding parallelism.\n",
    "\n",
    "Also feel free to refer to the in-class activity notebook for information or code examples (such as Pool/map) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced_value 174392717.6301426\n",
      "reduced_value 174442128.78514895\n",
      "reduced_value 174491513.74298924\n",
      "reduced_value 174540873.48921126\n",
      "reduced_value 174590208.42358\n",
      "reduced_value 174639518.84948972\n",
      "reduced_value 174688805.02044958\n",
      "reduced_value 174738067.1580947\n",
      "global_reduce 1396523833.099106\n",
      "CPU times: user 15.3 s, sys: 443 ms, total: 15.7 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Note we can use what is called a Jupyter Magic command to time the execution of this cell, which will allow us to tell if we are improving performance when adding parallelism.\n",
    "# Using the %%time command we can record the amount of execution time that it takes to execute this cell.\n",
    "\n",
    "# We will use the reduce function in functools.\n",
    "import functools\n",
    "# We use numpy for calculations\n",
    "import numpy as np\n",
    "# Use use Pool for parallel processing\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# This is a function that will be applied to each cell of our raster using the 'map' function\n",
    "def my_map(val):\n",
    "    #Let's just take the square root of the value in this example.\n",
    "    # This functionality could be swapped out for any type of operation you would like\n",
    "    return np.sqrt(val)\n",
    "\n",
    "# This is a function that will be applied to each of the mapped values using the 'functools.reduce' function\n",
    "def my_reduce(a,b):\n",
    "    # Let's just take the sum of the values in this example.\n",
    "    # Like map, this functionality could be swapped out.\n",
    "    return a+b\n",
    "\n",
    "# This function will process each tile array\n",
    "def process_tile_array(tile_array):\n",
    "    # This rather complex looking statement will flatten the 2D tile_array to a 1D array.\n",
    "    # Then we map the my_map function to each element in the 1D array.\n",
    "    # Next, we cast it to a list and back to a np.array (long story as to why)\n",
    "    mapped_grid = np.array(list(map(my_map,tile_array.flatten())))\n",
    "\n",
    "    # Now we apply our my_reduce function using the functools.reduce to the mapped_grid\n",
    "    reduced_value = functools.reduce(my_reduce,mapped_grid)\n",
    "    print(\"reduced_value\",reduced_value)\n",
    "    \n",
    "    return reduced_value\n",
    "\n",
    "# This function will process a file (when given a file number) by running process_tile_array\n",
    "def process_file(f_i):\n",
    "    ascii_grid = np.loadtxt(\"tmp_raster\"+str(f_i)+\".asc\",skiprows=6)\n",
    "    val = process_tile_array(ascii_grid)\n",
    "    return val\n",
    "\n",
    "\n",
    "def process_tiles(nfiles):\n",
    "    # We want to apply the reduce function across all the files\n",
    "    global_reduce = None\n",
    "\n",
    "    # Loop over all the number of files\n",
    "    for f_i in range(nfiles):\n",
    "        ascii_grid = np.loadtxt(\"tmp_raster\"+str(f_i)+\".asc\",skiprows=6)\n",
    "        val = process_tile_array(ascii_grid)\n",
    "        \n",
    "        if global_reduce is None:\n",
    "            global_reduce = val\n",
    "        else:\n",
    "            global_reduce = my_reduce(val, global_reduce)\n",
    "        \n",
    "    print(\"global_reduce\",global_reduce)\n",
    "\n",
    "process_tiles(nfiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: (2 points)\n",
    "Record the wall time for serial execution (seconds): Wall time: 15.8 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the Tile Array\n",
    "\n",
    "This stage will look at the finest granularity of parallelism.\n",
    "Parallelizing the the processing of each individual cell in our 2D raster tile array.\n",
    "\n",
    " 1. Create a Pool of 4 processes in the process_tile_array function\n",
    " 2. Replace the python map function with the map function from Pool to process the tile_array in parallel\n",
    " \n",
    "(Please note that there will be very few code changes to introduce parallelism in this stage. As a hint look at the in-class activity code for creating a pool and applying map to a pool of processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced_value 174392717.6301426\n",
      "reduced_value 174442128.78514895\n",
      "reduced_value 174491513.74298924\n",
      "reduced_value 174540873.48921126\n",
      "reduced_value 174590208.42358\n",
      "reduced_value 174639518.84948972\n",
      "reduced_value 174688805.02044958\n",
      "reduced_value 174738067.1580947\n",
      "global_reduce 1396523833.099106\n",
      "CPU times: user 8min 32s, sys: 11.5 s, total: 8min 43s\n",
      "Wall time: 9min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocess as mp\n",
    "\n",
    "# NOTE: Due to the following error, multiprocess was used instead of multiprocessing based on the fact that 'the multiprocessing module has a major limitation when \n",
    "# it comes to IPython use: Functionality within this package requires that the __main__ module be importable by the children. [...] This means that some examples, \n",
    "# such as the multiprocessing.pool.Pool examples will not work in the interactive interpreter. [from the documentation]. Fortunately, there is a fork of the \n",
    "# multiprocessing module called multiprocess which uses dill instead of pickle to serialization and overcomes this issue conveniently. \n",
    "# (https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror)\n",
    "\n",
    "# Process SpawnPoolWorker-23:\n",
    "# Traceback (most recent call last):\n",
    "#   File \"/Users/mbraaksma/mambaforge/envs/geovenv1/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
    "#     self.run()\n",
    "#   File \"/Users/mbraaksma/mambaforge/envs/geovenv1/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
    "#     self._target(*self._args, **self._kwargs)\n",
    "#   File \"/Users/mbraaksma/mambaforge/envs/geovenv1/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
    "#     task = get()\n",
    "#   File \"/Users/mbraaksma/mambaforge/envs/geovenv1/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
    "#     return _ForkingPickler.loads(res)\n",
    "# AttributeError: Can't get attribute 'my_map' on <module '__main__' (built-in)>\n",
    "\n",
    "# This is a function that will be applied to each cell of our raster using the 'map' function\n",
    "def my_map(val):\n",
    "    #Let's just take the square root of the value in this example.\n",
    "    # This functionality could be swapped out for any type of operation you would like\n",
    "    return np.sqrt(val)\n",
    "\n",
    "# This is a function that will be applied to each of the mapped values using the 'functools.reduce' function\n",
    "def my_reduce(a,b):\n",
    "    # Let's just take the sum of the values in this example.\n",
    "    # Like map, this functionality could be swapped out.\n",
    "    return a+b\n",
    "\n",
    "# This function will process each tile array\n",
    "def process_tile_array(tile_array):\n",
    "    \n",
    "    # Use a Pool to parallelize the mapping step\n",
    "    with mp.Pool(2) as pool:\n",
    "        # Flatten the 2D array to a 1D array and map `my_map` in parallel\n",
    "        mapped_grid = pool.map(my_map, tile_array.flatten())\n",
    "    \n",
    "    # This rather complex looking statement will flatten the 2D tile_array to a 1D array.\n",
    "    # Then we map the my_map function to each element in the 1D array.\n",
    "    # Next, we cast it to a list and back to a np.array (long story as to why)\n",
    "    # mapped_grid = np.array(list(map(my_map,tile_array.flatten())))\n",
    "\n",
    "    # Now we apply our my_reduce function using the functools.reduce to the mapped_grid\n",
    "    reduced_value = functools.reduce(my_reduce,mapped_grid)\n",
    "\n",
    "    print(\"reduced_value\", reduced_value)\n",
    "    return reduced_value\n",
    "\n",
    "# This function will process a file (when given a file number) by running process_tile_array\n",
    "def process_file(f_i):\n",
    "    ascii_grid = np.loadtxt(\"tmp_raster\"+str(f_i)+\".asc\",skiprows=6)\n",
    "    val = process_tile_array(ascii_grid)\n",
    "    return val\n",
    "\n",
    "\n",
    "def process_tiles(nfiles):\n",
    "    # We want to apply the reduce function across all the files\n",
    "    global_reduce = None\n",
    "\n",
    "    # Loop over all the number of files\n",
    "    for f_i in range(nfiles):\n",
    "        ascii_grid = np.loadtxt(\"tmp_raster\"+str(f_i)+\".asc\",skiprows=6)\n",
    "        val = process_tile_array(ascii_grid)\n",
    "        \n",
    "        if global_reduce is None:\n",
    "            global_reduce = val\n",
    "        else:\n",
    "            global_reduce = my_reduce(val, global_reduce)\n",
    "        \n",
    "    print(\"global_reduce\",global_reduce)\n",
    "\n",
    "process_tiles(nfiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: (2 points)\n",
    "Record the wall time for parallel times using a 4 pooled processes launched in process_tile_array: 9min 14s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: (2 points)\n",
    "Is this wall time what you expected? Why or why not? \n",
    "\n",
    "I suspected that it would not be faster and might even be slower because it has to start the parallel processing many times. However, I did not expect it to be **that** much slower, from 15 seconds to nearly 10 minutes. That is a shocking difference and it makes me wonder if I made a mistake somewhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: (4 points)\n",
    "Change the number of parallel process from 4 to 2. \n",
    "\n",
    "Record the time using 2 pool processes (2 points): 9min 51s\n",
    "\n",
    "Write a one sentence summary explaining why there might be a difference in execution times (3 points): The difference, although minimal, would be due to the fact that fewer processes are being used here so it takes slightly longer to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's move Pool\n",
    "\n",
    "If you notice, we starting up a new Pool of processors for every raster tile we process.\n",
    "This may be hurting our performance.\n",
    "So let's only launch one pool of processors and reuse it for each tile.\n",
    "In theory this should be faster so lets test it.\n",
    "\n",
    " 1. Move the Pool() function from process_tile_array to process_tiles\n",
    " 2. Alter the code so that the parallel map function in process_tile_array can use the pool from process_tiles. Hint: You will likely need to change the way the functions are defined.\n",
    " 3. Let's use 4 processors again, so change the pool size back to 4.\n",
    " 4. Run the newer version, verify the results are correct and record the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global reduced value: 1396523833.099106\n",
      "CPU times: user 8min 32s, sys: 10.7 s, total: 8min 42s\n",
      "Wall time: 9min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocess as mp\n",
    "\n",
    "# Define the mapping function to be applied to each cell\n",
    "def my_map(val):\n",
    "    return np.sqrt(val)\n",
    "\n",
    "# Define the reducing function to aggregate mapped results\n",
    "def my_reduce(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Modify process_tile_array to accept the pool as an argument\n",
    "def process_tile_array(tile_array, pool):\n",
    "    # Use pool.map to process the flattened array in parallel\n",
    "    mapped_grid = pool.map(my_map, tile_array.flatten())\n",
    "    \n",
    "    # Reduce the results to a single value\n",
    "    reduced_value = functools.reduce(my_reduce, mapped_grid)\n",
    "    return reduced_value\n",
    "\n",
    "# Main function to process all tiles \n",
    "def process_tiles(nfiles):\n",
    "    # Create the pool with 4 processes\n",
    "    with mp.Pool(4) as pool:\n",
    "        global_reduce = None\n",
    "        \n",
    "        # Loop over all files\n",
    "        for f_i in range(nfiles):\n",
    "            # Load the raster file\n",
    "            ascii_grid = np.loadtxt(f\"tmp_raster{f_i}.asc\", skiprows=6)\n",
    "            \n",
    "            # Process the tile array \n",
    "            val = process_tile_array(ascii_grid, pool)\n",
    "            \n",
    "            # Perform the reduction across tiles\n",
    "            if global_reduce is None:\n",
    "                global_reduce = val\n",
    "            else:\n",
    "                global_reduce = my_reduce(global_reduce, val)\n",
    "        \n",
    "        print(\"Global reduced value:\", global_reduce)\n",
    "\n",
    "process_tiles(nfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: (2 points)\n",
    "Record the wall time for parallel execution using a 4 pooled processes **launched in process_tiles**: 9min 17s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: (4 points)\n",
    "Is this wall time what you expected? Why or why not? \n",
    "\n",
    "This took almost exactly as much time as starting the pooled processes in the process_tile_array function. This makes sense because the nothing has functionally changed about the process, the parallel pools are being executed in the same way. The only difference is that the pools are being input into the function. It is still surprising to me that this level of parallel process takes so much longer than the serial process, but it makes sense that it matches the earlier parallel process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try processing entire files in parallel.\n",
    "\n",
    "Once we moved Pool to process_tiles we see that beautiful for loop processing each tile over the range of nfiles and we thought to ourselves - we can parallelize that loop!\n",
    "\n",
    " 1. Using the original serial template code (i.e., remove the parallel map code you just wrote) lets parallelize the file loop\n",
    " 2. Create a Pool of 4 processes in process_tiles\n",
    " 3. Change the process_tiles function to use the pool's parallel map function for all the files (nfiles).\n",
    "    (Hint: look at the unused process_file function)\n",
    " 4. Change the remaining code in process_tiles to accept the output from our parallel map function and still calculate the correct global_reduce.\n",
    "    (Hint: the for loops and structure of the code will need to change to add this level of parallelism)\n",
    " 5. Run the newer version, verify the results are correct and record the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced_valuereduced_valuereduced_valuereduced_value    174540873.48921126174442128.78514895174392717.6301426174491513.74298924\n",
      "\n",
      "\n",
      "\n",
      "reduced_value 174688805.02044958\n",
      "reduced_value 174738067.1580947\n",
      "reduced_value 174639518.84948972\n",
      "reduced_value 174590208.42358\n",
      "Global reduced value: 1396523833.099106\n",
      "CPU times: user 9.83 ms, sys: 35.1 ms, total: 45 ms\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def my_map(val):\n",
    "    return np.sqrt(val)\n",
    "\n",
    "def my_reduce(a,b):\n",
    "    return a+b\n",
    "\n",
    "def process_tile_array(tile_array):\n",
    "    mapped_grid = np.array(list(map(my_map,tile_array.flatten())))\n",
    "    reduced_value = functools.reduce(my_reduce,mapped_grid)\n",
    "    print(\"reduced_value\",reduced_value)\n",
    "    return reduced_value\n",
    "\n",
    "def process_file(f_i):\n",
    "    ascii_grid = np.loadtxt(\"tmp_raster\"+str(f_i)+\".asc\",skiprows=6)\n",
    "    val = process_tile_array(ascii_grid)\n",
    "    return val\n",
    "\n",
    "\n",
    "def process_tiles(nfiles):\n",
    "    # We want to apply the reduce function across all the files\n",
    "    global_reduce = None\n",
    "\n",
    "    # Create a list of file indices\n",
    "    file_indices = list(range(nfiles))\n",
    "    \n",
    "    # Use a single pool to parallelize the processing of files\n",
    "    with mp.Pool(4) as pool:\n",
    "        # Use pool.map to process each file in parallel\n",
    "        results = pool.map(process_file, file_indices)\n",
    "    \n",
    "    # Reduce the results from all files\n",
    "    global_reduce = functools.reduce(my_reduce, results)\n",
    "    print(\"Global reduced value:\", global_reduce)\n",
    "\n",
    "process_tiles(nfiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: (2 points)\n",
    "Record the wall time for parallel execution using 4 pooled processes to process the files in parallel: 4.42 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: (4 points)\n",
    "Is this wall time what you expected? Why or why not? \n",
    "\n",
    "I expected the wall time to be faster, but the difference is still glaring. This is now faster than the original serial process. Clearly, this is illustrating the fact the the placement of the parallelization is crucial and requires careful consideration. I knew this was important, but I never realized putting it in the wrong spot could also be so detrimental (assuming I did not make an error). Understanding the workflow and where the bottlenecks occur is key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: (4 points)\n",
    "Explain (in your own words) why processing files in parallel was different from processing numpy arrays in parallel. Does Amdahl's Law (https://cvw.cac.cornell.edu/Parallel/amdahl) provide any clues to explain the difference in execution times?\n",
    "\n",
    "Interacting with the file system is generally much slower than working with data already loaded into memory. NumPy is optimized for fast, in-memory calculations, so adding parallel processes often slowed things down due to extra setup time and the need to share data between processes. In contrast, loading files from disk is slow but can be sped up with parallel processing because each file can be read independently without dependencies. The key is to identify which part of the workflow is slow and focus parallel processing there. Amdahl's Law shows that the NumPy processing had a large serial portion, limiting speedup, while file processing was easier to parallelize due to minimal shared overhead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: (10 points)\n",
    "\n",
    "Open problem: Now that you have experience with parallelism using the multiprocessing module. Take some time to create your own parallel geospatial method. Feel free to use the code in the Activity and Lab sections. Examples include: FocalMean (Raster), Mean Center (Vector), etc. You have the freedom to make this a relatively simple method or a more complex method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from multiprocess import Pool\n",
    "from pyspatialstats.focal_stats import focal_std\n",
    "\n",
    "\n",
    "\n",
    "def process_tile(file_index):\n",
    "    # Load the raster data\n",
    "    ascii_grid = np.loadtxt(\"tmp_raster\"+str(file_index)+\".asc\",skiprows=6)\n",
    "    \n",
    "    # Apply the focal standard deviation function\n",
    "    processed_tile = focal_std(ascii_grid, window=3, fraction_accepted=0.7, std_df=0)\n",
    "    \n",
    "    return processed_tile\n",
    "\n",
    "# Main function to process all tiles using a single pool\n",
    "def process_tiles_in_parallel(nfiles, num_processes, parallel=True):\n",
    "\n",
    "    if parallel == False:\n",
    "        for file_index in range(nfiles):\n",
    "            ascii_grid = np.loadtxt(\"tmp_raster\"+str(file_index)+\".asc\",skiprows=6)\n",
    "            result = processed_tile = focal_std(ascii_grid, window=3, fraction_accepted=0.7, std_df=0)\n",
    "        print(f\"Focal STD Complete for serial processes.\")\n",
    "\n",
    "    else:\n",
    "        file_indices = list(range(nfiles))\n",
    "        with Pool(num_processes) as pool:\n",
    "            results = pool.map(process_tile, file_indices)\n",
    "            print(f\"Focal STD Complete for {num_processes} processes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal STD Complete for 2 processes.\n",
      "CPU times: user 80.1 ms, sys: 349 ms, total: 429 ms\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_tiles_in_parallel(nfiles, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal STD Complete for 4 processes.\n",
      "CPU times: user 87.2 ms, sys: 380 ms, total: 467 ms\n",
      "Wall time: 841 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_tiles_in_parallel(nfiles, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal STD Complete for serial processes.\n",
      "CPU times: user 1.53 s, sys: 127 ms, total: 1.65 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_tiles_in_parallel(nfiles, num_processes=None, parallel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: (4 points)\n",
    "    \n",
    "Run your new parallel geospatial method using 2 and 4 processes and record the wall time. Is this wall time what you expected?\n",
    "\n",
    "The wall time is what I would expect, increasing from serial to 2 processes to 4 processes. This pattern makes sense because I set up the code to parallelize the section where file processing is the highest, which benefits significantly from parallel execution. Since this part of the code doesn't have interdependencies between files, it scales well with the addition of more processes. As a result, the speedup from parallel processing is fairly unsurprising, especially given that the overhead associated with managing multiple processes is outweighed by the gains in efficiency for these file reading operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geovenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
